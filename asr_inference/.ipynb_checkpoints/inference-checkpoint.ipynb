{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e181788-db29-4f43-b74f-e2f6a1613063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from espnet2.bin.asr_inference import Speech2Text\n",
    "import soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59c4ea-3b94-4193-b058-2d56283049ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet2.bin.asr_inference_TruCLeS import Speech2Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabbbba5-259c-4fe7-99c8-a37855345506",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6418e490-ca07-4896-ab74-d3cab4271e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, rate = librosa.load(\"/hdd_storage/data/ASR_datasets/LJSpeech-1.1/LJSpeech-1.1/wavs/LJ001-0007.wav\", sr=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5b202-d53d-4462-929b-9d71d681c23c",
   "metadata": {},
   "source": [
    "speech2text(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7bc8dd-e09f-49dd-a848-544379c69d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "634de04e-2757-402d-bd80-93904d5f5161",
   "metadata": {},
   "source": [
    "audio, rate = soundfile.read(\"/hdd_storage/data/ASR_datasets/IISc_Miles/tamil/mile_tamil_asr_corpus/test/audio_files/MICI_0000000_0000004.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f71c52-20fc-49ce-a127-f2bc52828408",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_config_path = \"/home/rathna/expts/en_models_espnet/models--asapp--e_branchformer_librispeech/snapshots/f50914447c48b091738b3e020023ac69dbde9ea9/exp/asr_train_asr_e_branchformer_raw_en_bpe5000_sp/config.yaml\"\n",
    "asr_model_path = \"/home/rathna/expts/en_models_espnet/models--asapp--e_branchformer_librispeech/snapshots/f50914447c48b091738b3e020023ac69dbde9ea9/exp/asr_train_asr_e_branchformer_raw_en_bpe5000_sp/valid.acc.ave_10best.pth\"\n",
    "\n",
    "lm_config_path = \"/home/rathna/expts/en_models_espnet/models--asapp--e_branchformer_librispeech/snapshots/f50914447c48b091738b3e020023ac69dbde9ea9/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml\"\n",
    "lm_model_path = \"/home/rathna/expts/en_models_espnet/models--asapp--e_branchformer_librispeech/snapshots/f50914447c48b091738b3e020023ac69dbde9ea9/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/valid.loss.ave_10best.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46f684f-64d5-43dd-8fef-0fd1c29fc621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet2.tasks.asr_TruCLeS import ASRTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17953a77-efa2-4f93-879b-c67694186008",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model, asr_train_args = ASRTask.build_model_from_file(asr_config_path, asr_model_path,\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4cd9f18-bdec-4ebf-a4fd-3412f07db8b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESPnetASRModel(\n",
       "  (frontend): DefaultFrontend(\n",
       "    (stft): Stft(n_fft=512, win_length=512, hop_length=160, center=True, normalized=False, onesided=True)\n",
       "    (frontend): Frontend()\n",
       "    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)\n",
       "  )\n",
       "  (specaug): SpecAug(\n",
       "    (time_warp): TimeWarp(window=5, mode=bicubic)\n",
       "    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)\n",
       "    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=10, axis=time)\n",
       "  )\n",
       "  (normalize): GlobalMVN(stats_file=/home/rathna/expts/en_models_espnet/models--asapp--e_branchformer_librispeech/snapshots/f50914447c48b091738b3e020023ac69dbde9ea9/exp/asr_stats_raw_en_bpe5000_sp/train/feats_stats.npz, norm_means=True, norm_vars=True)\n",
       "  (encoder): EBranchformerEncoder(\n",
       "    (embed): Conv2dSubsampling(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (out): Sequential(\n",
       "        (0): Linear(in_features=9728, out_features=512, bias=True)\n",
       "        (1): RelPositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoders): MultiSequential(\n",
       "      (0): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (1): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (2): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (3): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (4): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (5): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (6): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (7): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (8): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (9): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (10): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (11): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (12): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (13): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (14): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (15): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (16): EBranchformerEncoderLayer(\n",
       "        (attn): RelPositionMultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (cgmlp): ConvolutionalGatingMLP(\n",
       "          (channel_proj1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "          )\n",
       "          (csgu): ConvolutionalSpatialGatingUnit(\n",
       "            (norm): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "            (conv): Conv1d(1536, 1536, kernel_size=(31,), stride=(1,), padding=(15,), groups=1536)\n",
       "            (act): Identity()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (channel_proj2): Linear(in_features=1536, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (feed_forward_macaron): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): Swish()\n",
       "        )\n",
       "        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_mlp): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm_final): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (depthwise_conv_fusion): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024)\n",
       "        (merge_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embed): Sequential(\n",
       "      (0): Embedding(5000, 512)\n",
       "      (1): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (output_layer): Linear(in_features=512, out_features=5000, bias=True)\n",
       "    (decoders): MultiSequential(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (criterion_att): LabelSmoothingLoss(\n",
       "    (criterion): KLDivLoss()\n",
       "  )\n",
       "  (ctc): CTC(\n",
       "    (ctc_lo): Linear(in_features=512, out_features=5000, bias=True)\n",
       "    (ctc_loss): CTCLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "asr_model.to(dtype=getattr(torch, dtype)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20dc478c-d4fc-4d7f-a60b-af641caeb9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = \"float32\"\n",
    "device = \"cuda:0\"\n",
    "beam_size = 10\n",
    "ctc_weight = 0.5\n",
    "lm_weight = 1.0\n",
    "ngram_weight = 0.9\n",
    "penalty = 0.0\n",
    "weights = weights = dict(\n",
    "                decoder=1.0 - ctc_weight,\n",
    "                ctc=ctc_weight,\n",
    "                lm=lm_weight,\n",
    "                ngram=ngram_weight,\n",
    "                length_bonus=penalty,\n",
    "            )\n",
    "scorers = {}\n",
    "sos = asr_model.sos\n",
    "eos = asr_model.eos\n",
    "token_list = asr_model.token_list\n",
    "vocab_size = len(token_list)\n",
    "maxlenratio = 0.0\n",
    "minlenratio = 0.0\n",
    "nbest = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbe03ec2-f8a0-408e-8b31-6f065bd1d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet.nets.beam_search import BeamSearch, Hypothesis\n",
    "beam_search = BeamSearch(\n",
    "                    beam_size=beam_size,\n",
    "                    weights=weights,\n",
    "                    scorers=scorers,\n",
    "                    sos=asr_model.sos,\n",
    "                    eos=asr_model.eos,\n",
    "                    vocab_size=len(token_list),\n",
    "                    token_list=token_list,\n",
    "                    pre_beam_score_key=None if ctc_weight == 1.0 else \"full\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38de4e61-cd56-4c73-8c6f-b4b5fff3f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet2.torch_utils.device_funcs import to_device\n",
    "from espnet2.text.build_tokenizer import build_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65063921-5b5d-4d9b-9aa1-7371ef804a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = torch.tensor(audio)\n",
    "speech = speech.unsqueeze(0).to(getattr(torch, dtype))\n",
    "lengths = speech.new_full([1], dtype=torch.long, fill_value=speech.size(1))\n",
    "batch = {\"speech\": speech, \"speech_lengths\":lengths}\n",
    "batch = to_device(batch, device=device)\n",
    "enc, enc_olens = asr_model.encode(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef3a8279-fd36-40d8-9b7f-8f1cc560408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([[[-0.1433, -0.1154,  0.3538,  ...,  0.0408, -0.1262, -0.4942],\n",
      "         [-0.0223, -0.0131,  0.0950,  ..., -0.0078,  0.0017, -0.0537],\n",
      "         [-0.1283, -0.1926,  0.2731,  ..., -0.1386, -0.0154, -0.2468],\n",
      "         ...,\n",
      "         [-0.5474,  0.2824,  0.0833,  ..., -0.1272, -0.7413,  0.2022],\n",
      "         [-0.0676,  0.0016,  0.0236,  ..., -0.0020, -0.0512,  0.0239],\n",
      "         [-0.0500,  0.0056,  0.0156,  ...,  0.0055, -0.0318,  0.0145]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>) torch.Size([1, 209, 512])\n"
     ]
    }
   ],
   "source": [
    "print(isinstance(enc,tuple))\n",
    "print(enc, enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119a31d-c203-4f95-af3e-575962ecbca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet2.asr.transducer.beam_search_transducer import Hypothesis as TransHypothesis\n",
    "def decode_single_sample(enc,beam_search):\n",
    "    nbest_hyps = beam_search(\n",
    "        x=enc, maxlenratio=maxlenratio, minlenratio=minlenratio\n",
    "    )\n",
    "\n",
    "    nbest_hyps = nbest_hyps[: self.nbest]\n",
    "\n",
    "    results = []\n",
    "    for hyp in nbest_hyps:\n",
    "        assert isinstance(hyp, (Hypothesis, TransHypothesis)), type(hyp)\n",
    "\n",
    "        # remove sos/eos and get results\n",
    "        last_pos = None if asr_model.use_transducer_decoder else -1\n",
    "        if isinstance(hyp.yseq, list):\n",
    "            token_int = hyp.yseq[1:last_pos]\n",
    "        else:\n",
    "            token_int = hyp.yseq[1:last_pos].tolist()\n",
    "\n",
    "        # remove blank symbol id, which is assumed to be 0\n",
    "        token_int = list(filter(lambda x: x != 0, token_int))\n",
    "\n",
    "        # Change integer-ids to tokens\n",
    "        token = self.converter.ids2tokens(token_int)\n",
    "\n",
    "        if self.tokenizer is not None:\n",
    "            text = self.tokenizer.tokens2text(token)\n",
    "        else:\n",
    "            text = None\n",
    "        results.append((text, token, token_int, hyp))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7993a26-15e8-4bb9-973a-8c08b9e4c185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a4f3d-1f83-4658-8a69-961197110361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3fc06-f9f9-4f5d-85b7-28874cccf690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa163227-8f11-4d4b-b059-d4575e882f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.converter = converter\n",
    "self.tokenizer = tokenizer\n",
    "self.beam_search = beam_search\n",
    "self.beam_search_transducer = beam_search_transducer\n",
    "self.hugging_face_model = hugging_face_model\n",
    "self.hugging_face_linear_in = hugging_face_linear_in\n",
    "self.hugging_face_decoder_conf = hugging_face_decoder_conf\n",
    "self.maxlenratio = maxlenratio\n",
    "self.minlenratio = minlenratio\n",
    "self.nbest = nbest\n",
    "self.enh_s2t_task = enh_s2t_task\n",
    "self.multi_asr = multi_asr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
